{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "**DUE NOV 11th at 11:59 PM**\n",
    "\n",
    "You are expected to turn in a **pdf version** of this notebook with all your **codes, results, and figures** (Use the print option). Make sure the figures and results are visible as you want them to appear in the pdf before turning it in. Please do not modify the instructions as doing so will limit our ability to follow and grade your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (HW1 Problem 4)\n",
    "\n",
    "In this problem, you will work on the clustering problem using Bottom-up Agglomerative clustering and K-mean clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) A 4-D dataset is given in **'iris.csv'** with the last column being the ground truth label. Load the file. Store the data in a variable ***X*** and store the label in a variable ***y***. Because clustering is an unsupervised task, there is no need for the labels during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Train a clustering model using Bottom-up Agglomerative clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your clusters on a 2-D plot. Choose any 2 dimensions from the 4 dimensions to plot. Try to pick the 2 dimensions that best separate the data. Your plot should contains all the data points with points from the same predicted cluster haveing the same color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the visualization step above using the same 2 dimensions. This time, plot according to the ground truth classes. Comment on the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Train a clustering model using K-mean clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your clusters on a 2-D plot. Choose any 2 dimensions from the 4 dimensions to plot. Try to pick the 2 dimensions that best separate the data. Your plot should contains all the data points with points from the same predicted cluster haveing the same color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the visualization step above using the same 2 dimensions. This time, plot according to the ground truth classes. Comment on the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Perform Principle Component Analysis (PCA) on the data. Project the original data on the 2 largest principle components. Store this new projected 2-D data in a variable ***X_projected***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat part **b** on the new 2-D data. Train the Bottom-up Agglomerative model and visualize your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat part **c** on the new 2-D data. Train the K-means model and visualize your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the quality of 4-D and 2-D clusterings. When would the ideas of projection and dimensionality reduction be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "In this problem, you will first implement the Naive Bayes (NB) algorithm from scratch. We will use a dataset for classifying if a patient has breast cancer. Each instance (row) in the dataset is a patient described by the 9 following features (with their corresponding values).\n",
    "\n",
    "1. Clump Thickness: 1 - 10\n",
    "2. Uniformity of Cell Size: 1 - 10\n",
    "3. Uniformity of Cell Shape: 1 - 10\n",
    "4. Marginal Adhesion: 1 - 10\n",
    "5. Single Epithelial Cell Size: 1 - 10\n",
    "6. Bare Nuclei: 1 - 10\n",
    "7. Bland Chromatin: 1 - 10\n",
    "8. Normal Nucleoli: 1 - 10\n",
    "9. Mitoses: 1 - 10\n",
    "\n",
    "Given these features, we will classify a car into one of the 2 classes: 0 (benign) or 1 (malignant).\n",
    "\n",
    "NB is a very simple algorithm. Consider a feature **X**. For each value **$x_i$** of **X** and each class label **$y_j$**, NB calculates the value of $P(X = x_i | Y = y_j)$. For example, take the feature **Mitoses**, NB will calculate all the following values:\n",
    "\n",
    "- P(Mitoses = 1 | Class = 0), P(Mitoses = 2 | Class = 0), P(Mitoses = 3 | Class = 0), ... , P(Mitoses = 10 | Class = 0)\n",
    "- P(Mitoses = 1 | Class = 1), P(Mitoses = 2 | Class = 1), P(Mitoses = 3 | Class = 1), ... , P(Mitoses = 10 | Class = 1)\n",
    "\n",
    "Repeat this calculation for all the features. In the end, NB keep a recording of all possible $P(X | Y)$. The calculation itself is intuitive:\n",
    "\n",
    "$$\n",
    " P(X = x_i | Y = y_j) = \\frac{\\text{Number of rows with } X = x_i \\text{ and } Y = y_j}{\\text{Number of rows with } Y = y_j}\n",
    "$$\n",
    "\n",
    "In addition, NB also calculate the priors probability $P(Y = y_j)$. Again, intuitively:\n",
    "\n",
    "$$\n",
    " P(Y = y_i) = \\frac{\\text{Number of rows with } Y = y_j}{\\text{Number of rows in the dataset}}\n",
    "$$\n",
    "\n",
    "Given a test example $X_{test} = \\{ X_0 = x_0, X_1 = x_1, \\ldots, X_i = x_i\\}$, for each class label $y_j$, NB calculate:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " P(Y = y_j | X_{test}) & = P(X_{test} | Y = y_j)P(Y = y_j) \\\\\n",
    "                       & = P(X_0 = x_0 | Y = y_j)P(X_1 = x_1 | Y = y_j) \\ldots P(X_i = x_i | Y = y_j)P(Y = y_j)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Such calculation is easy since we have bookkept all $P(X | Y)$ and all $P(Y)$ in previous steps. The output of the model is simply:\n",
    "\n",
    "$$\n",
    " \\underset{y_j}{\\operatorname{argmax}} P(Y = y_j | X_{test})\n",
    "$$\n",
    "\n",
    "You will do each of these steps following this problem. We will use Pandas to deal with the data in this problem. Pandas can do queries like \"Get all the rows in which Clump Thickness = 2 and Class = 1\" with minimal syntax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) First let's load the dataset and store it in a Pandas dataframe. Play with the dataframe and get used to the queries (this part is not graded). This guide is a good place to start:\n",
    "\n",
    "https://medium.com/python-in-plain-english/filtering-rows-and-columns-in-pandas-python-techniques-you-must-know-6cdfc32c614c \n",
    "\n",
    "Split the dataset into a training set and a testing set. Use 10% of the data as the testing set (The splitting is graded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Now we will build up the bookkeeping. Let's start with the priors $P(Y = y_j)$, $y_j \\in$ {0,1}. For each of these labels, calculate $P(Y = y_j)$ by simply counting the number of times $y_j$ appears in the dataset divided by the size of the dataset. You can bookkeep the priors in a dictionary with keys being $y_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the conditional probability $P(X = x_i | Y = y_j)$. If you do the query with Pandas, this counting should be simple. This time, store the conditional probability in another dictionary with keys ($x_i$, $y_j$) or any data structure that you prefer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You have successfully \"trained\" a NB model. Let's test our model on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a **predict** function that returns $\\underset{y_j}{\\operatorname{argmax}} P(Y = y_j | X_{test})$ (refer to the description above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the label of all the instances in the test dataset, calculate and print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "In this problem, you will implement the Logistic Regression (LR) algorithm from scratch. Similar to NB, LR relies on $P(Y|X)$ to predict the class of an example. However, unlike NB, a generative model, LR is a discriminative model so it does not need to estimate $P(X|Y)$ and $P(Y)$. LR assumes the form of the conditional probability $P(Y|X)$ to be:\n",
    "\n",
    "$$\n",
    "    P(Y | X) = f(X) = \\frac{1}{1 + e^{-(\\alpha X + \\beta)}}\n",
    "$$\n",
    "\n",
    "$f(X)$ returns a value in (0,1). The model classifies $X$ as $1$ if $f(X)$ is closer to $1$ and $0$ otherwise. We have to estimate the model parameters: the vector $\\alpha$ and $\\beta$ from the data, which we will do via stochastic gradient descent (SGD). In SGD, a training example is shown to the model each at a time. The model makes a prediction on the training example and the error between the prediction and the ground-truth label is used to update the model's parameters. We use the log-likelihood loss to estimate the error in this problem. In particular, the log-likelihood loss for classifying the $X^{(i)}$ example with the ground-truth $y^{(i)}$ is: \n",
    "\n",
    "$$\n",
    "    LL(y^{(i)},f(X^{(i)})) = -(y^{(i)} \\log(f(X^{(i)})) + (1 - y^{(i)}) \\log(1 - f(X^{(i)})))\n",
    "$$\n",
    "\n",
    "In our case, the updating is as the followings:\n",
    "\n",
    "$$\n",
    "\\alpha_{j}(t+1) = \\alpha_{j}(t) - \\frac{\\partial LL}{\\partial \\alpha_j} \\times rate\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta(t+1) = \\beta(t) - \\frac{\\partial LL}{\\partial \\beta} \\times rate\n",
    "$$\n",
    "\n",
    "where $rate$ is the how much change we want to make to the parameters in each update. Whenever we finish looping through all the instances in the dataset to update the parameters, we finish a training epoch. We may do many training epochs (looping through the dataset many times), until the model parameters converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to derive the formula for $\\frac{\\partial LL}{\\partial \\alpha_j}$ and $\\frac{\\partial LL}{\\partial \\beta}$. To keep the problem simple, these are provided to you:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial LL}{\\partial \\alpha_j} = -(y^{(i)} - f(X^{(i)}))X^{(i)}_{j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial LL}{\\partial \\beta} = -(y^{(i)} - f(X^{(i)}))\n",
    "$$\n",
    "\n",
    "a) We use the same dataset for predicting breast cancer in Problem 2 for this problem. Load the dataset and split it into a training set and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Learn the model parameters using SGD. A skeleton for SGD is provided to help you understand the process (You don't have to use the provided code). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "rate = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # for instance in traning dataset:\n",
    "        # Update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Load the testing dataset. Use $f(X)$ to predict the labels of the test instances. Calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "In this problem, you will investigate how the size of the training data on the performance of a generative model (NB) and a discriminative model (LR). **You are free to use scikit-learn's NB and LR.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Load the breast cancer training dataset and split it into a training set and a testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) We will train NB classifiers that use only a portion of the training data. Let's vary this portion from 1% to 100% of the training set (100 iterations). For each portion value, sample the training set according to the portion, train a NB classifer on this portion of the training set, and evaluate the model accuracy on the testing set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accuracies of the models versus the portion of the training set used. Comment on the effect of increasing the size of the training dataset on the performance of NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Repeat part **b** with LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between a generative model (NB) and a discriminative model (LR), which is more affected by small training dataset? Think about a reason for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "What is the hyperparameters, the parameters, and the objective function for finding the best hypothesis for each of the following methods:\n",
    "\n",
    "1. Decision Tree (Entropy gain)\n",
    "2. Support Vector Machine\n",
    "3. K-nearest Neighbors Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
